# environment
task: 'xarm_lift'
modality: 'state'
action_repeat: ???
discount: ???
episode_length: ???
train_steps: ???
reward_scale: 1.0

# planning
mpc: true
iterations: 1
num_samples: 512
num_elites: 50
mixture_coef: 0.1
min_std: 0.05
max_std: 2.0
temperature: 0.5
momentum: 0.1
uncertainty_cost: ???

# actor
log_std_min: -10
log_std_max: 2

# learning
batch_size: 256
max_buffer_size: ???
horizon: 5
reward_coef: 0.5
value_coef: 0.1
consistency_coef: 20
rho: 0.5
kappa: 0.1
lr: 3e-4
std_schedule: ${min_std}
horizon_schedule: ${horizon}
per: true
per_alpha: 0.6
per_beta: 0.4
grad_clip_norm: 10
seed_steps: 0
update_freq: 2
tau: 0.01
utd: 1

# offline rl
dataset_dir: ???
data_first_percent: 1.0
is_data_clip: true
data_clip_eps: 1e-5
expectile: 0.7
A_scaling: 3.0

# offline->online
offline_steps: ${train_steps}/2
pretrained_model_path: ""
balanced_sampling: true
demo_schedule: 0.5

# architecture
enc_dim: 256
num_q: 5
mlp_dim: 512
latent_dim: 50

# MODEM
modem: false

# wandb
use_wandb: true
wandb_project: FOWM
wandb_entity: fyhmer  # insert your own

# misc
device: cuda
buffer_device: cuda
seed: 1
exp_name: default
notes: ""
eval_freq: 20000
save_freq: 50000
eval_episodes: 20
save_video: false
save_model: false
save_buffer: false
