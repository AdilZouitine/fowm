# simxarm
Minimal xArm simulation environment with a gym-style API.


## Installation

The package can be installed using pip:
```
pip install git+https://github.com/nicklashansen/simxarm.git@main#egg=simxarm
```

Alternatively, you can clone the repository and install an editable version locally:
```
git clone git@github.com:nicklashansen/simxarm.git
cd simxarm
pip install -e .
```

Most dependencies should be installed automatically. However, you may need to install [MuJoCo 2.1.0](https://github.com/deepmind/mujoco/releases/tag/2.1.0) if you do not already have it installed. To install MuJoCo, download it on the link above and make sure it is located at `~/.mujoco/mujoco210`. Then, add the following lines to your `~/.bashrc` file:
```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin
```
and similarly place your MuJoCo license key at `~/.mujoco/mjkey.txt`. A free and unrestricted license key can be obtained [here](https://www.roboti.us/file/mjkey.txt).


## Usage

The package provides a gym-style API for controlling the xArm simulation environment. The environment can be initialized as follows:
```
import simxarm
env = simxarm.make(task)
```
where `task` is a string specifying the task to be performed. The following tasks are currently supported:
```
Tasks:
OrderedDict([('reach', {'env': <class 'simxarm.task.reach.Reach'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Reach a target location with the end effector'}), ('push', {'env': <class 'simxarm.task.push.Push'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Push a cube to a target location'}), ('peg-in-box', {'env': <class 'simxarm.task.peg_in_box.PegInBox'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Insert a peg into a box'}), ('lift', {'env': <class 'simxarm.task.lift.Lift'>, 'action_space': 'xyzw', 'episode_length': 50, 'description': 'Lift a cube above a height threshold'})])

Initialized environment: reach
Observation space: (14,)
Action space: (3,)

Initialized environment: push
Observation space: (35,)
Action space: (3,)

Initialized environment: peg-in-box
Observation space: (35,)
Action space: (3,)

Initialized environment: lift
Observation space: (28,)
Action space: (4,)

Input types:
Observation space state: (28,)
Observation space rgb: (3, 84, 84)
Observation space all: [(3, 84, 84), (4,)]

```
The output above is generated by running `python test.py`. You can verify your installation by running this script.

The `simxarm.make` function is your gateway to the environment. It takes the following arguments:
```
"""
Create a new environment.
Args:
    task (str): The task to create an environment for. Must be one of:
        - 'reach'
        - 'push'
        - 'peg-in-box'
        - 'lift'
    obs_mode (str): The observation mode to use. Must be one of:
        - 'state': Ground-truth state observations
        - 'rgb': RGB images
        - 'all': RGB images and robot state observations
    image_size (int): The size of the image observations
    seed (int): The random seed to use
Returns:
    gym.Env: The environment
""" 
```
The above docstring is taken directly from the code.

To get started with the `simxarm` package, try play around with the `test.py` script and related files. Check that you can run the script without any errors. Then, try changing the task, observation mode, and image size. You can also try to execute random actions in the environment and visualize (render) the observations.


## Acknowledgements
This repository is based on work by [Nicklas Hansen](https://nicklashansen.github.io/), [Yanjie Ze](https://yanjieze.com/), [Rishabh Jangir](https://jangirrishabh.github.io/), [Mohit Jain](https://natsu6767.github.io/), and [Sambaran Ghosal](https://github.com/SambaranRepo) as part of the following publications:
* [Self-Supervised Policy Adaptation During Deployment](https://arxiv.org/abs/2007.04309)
* [Generalization in Reinforcement Learning by Soft Data Augmentation](https://arxiv.org/abs/2011.13389)
* [Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation](https://arxiv.org/abs/2107.00644)
* [Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation](https://arxiv.org/abs/2201.07779)
* [Visual Reinforcement Learning with Self-Supervised 3D Representations](https://arxiv.org/abs/2210.07241)
